{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768ff994-512a-4d3a-b971-027fa17ec771",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "--\n",
    "---\n",
    "Gradient Boosting Regression is a machine learning technique that belongs to the class of ensemble methods, specifically falling under the category of boosting algorithms. Gradient Boosting is used for both classification and regression tasks, but when it's applied to regression problems, it's referred to as Gradient Boosting Regression.\n",
    "\n",
    "The general idea behind Gradient Boosting Regression is to sequentially build a series of weak learners, usually decision trees, and combine them to create a strong predictive model. Unlike traditional decision tree models, which are built in parallel, boosting algorithms build trees sequentially, with each tree aiming to correct the errors made by the previous ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e61f5-7380-43f2-890d-c7c9804ff9e7",
   "metadata": {},
   "source": [
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's  performance using metrics such as mean squared error and R-squared.\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70dec2b4-960f-40cd-9053-70f38843dd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.00\n",
      "R2 Score: 0.92\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = X[:80], X[80:], y[:80], y[80:]\n",
    "\n",
    "class GradientBoosting:\n",
    "    def __init__(self, S=5, learning_rate=1, max_depth = 1, min_samples_split = 2, regression=True):\n",
    "        self.S = S\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.regression=regression\n",
    "        \n",
    "        \n",
    "        tree_params = {'max_depth': self.max_depth, 'min_samples_split': self.min_samples_split}\n",
    "        self.models = [DecisionTreeRegressor(**tree_params) for _ in range(S)]        \n",
    "   \n",
    "    def grad(self, y, h):\n",
    "        return y - h\n",
    "\n",
    "    def fit(self, X, y):  \n",
    "        # Fit the first mode\n",
    "        self.models[0].fit(X, y)\n",
    "        \n",
    "        for i in range(self.S - 1):\n",
    "\n",
    "            yhat = self.predict(X, self.models[:i+1], with_argmax=False)\n",
    "            \n",
    "            # Get the gradient\n",
    "            gradient = self.grad(y, yhat)\n",
    "            \n",
    "            # Fit the next model with gradient\n",
    "            self.models[i+1].fit(X, gradient)\n",
    "    \n",
    "    def predict(self, X, models=None, with_argmax=True):\n",
    "        if models is None:\n",
    "            models = self.models\n",
    "        h0 = models[0].predict(X)  # first use the initial model\n",
    "        boosting = sum(self.learning_rate * model.predict(X) for model in models[1:])\n",
    "        yhat = h0 + boosting\n",
    "        if not self.regression:\n",
    "            # Turn into probability distribution (Softmax)\n",
    "            yhat = np.exp(yhat) / np.sum(np.exp(yhat), axis=1, keepdims=True)\n",
    "            if with_argmax:\n",
    "                yhat = np.argmax(yhat, axis=1)\n",
    "        return yhat\n",
    "\n",
    "# Initialize model\n",
    "model = GradientBoosting(S=200, learning_rate=0.1, max_depth = 3, min_samples_split = 2)\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'MSE: {mse:.2f}')\n",
    "print(f'R2 Score: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e087cad-2192-4d5c-b814-87f8fe026728",
   "metadata": {},
   "source": [
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "--\n",
    "---\n",
    "In the context of gradient boosting, a weak learner is a simple machine learning model that is only slightly better than random guessing. Weak learners are typically used in ensemble learning algorithms, where they are combined to create a more accurate and powerful prediction model.\n",
    "\n",
    "The use of weak learners in gradient boosting is based on the idea that it is easier to train a model to make small improvements than to train a model to make large improvements. By combining many weak learners, each of which makes a small improvement, it is possible to create a model that is much more accurate than any of the individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05de1386-2b8f-45cc-a837-e71c826c4651",
   "metadata": {},
   "source": [
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "--\n",
    "---\n",
    "The intuition behind the Gradient Boosting algorithm is to combine several weak learners to create a strong learner that can make accurate predictions. Here's a step-by-step breakdown of the intuition:\n",
    "\n",
    "1. **Start with a Weak Learner**: The algorithm starts with a weak learner, typically a decision tree, which is just slightly better than random guessing.\n",
    "\n",
    "2. **Calculate Errors**: The algorithm calculates the errors or residuals, which are the differences between the actual and predicted values.\n",
    "\n",
    "3. **Train Another Weak Learner**: The algorithm then trains another weak learner to predict these residuals instead of the actual values. This new weak learner is therefore focused on correcting the mistakes of the first weak learner.\n",
    "\n",
    "4. **Update Predictions**: The predictions of the model are updated using the predictions of this new weak learner. This means that the new weak learner's predictions are added to the previous weak learner's predictions to get the final prediction.\n",
    "\n",
    "5. **Repeat**: Steps 2-4 are repeated for a specified number of iterations. With each iteration, a new weak learner is trained to correct the residuals of the previous weak learner.\n",
    "\n",
    "6. **Final Model**: The final model is a weighted sum of all the weak learners. Each weak learner in the ensemble focuses on the errors of the previous one, thereby improving the predictions of the model with each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1efb19-9f24-4cb1-9e59-f1b7ab0eb55b",
   "metadata": {},
   "source": [
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "--\n",
    "---\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners in a stage-wise fashion. Here's how it works:\n",
    "\n",
    "1. **Initialize the Model**: The algorithm starts by predicting the average of the target variable.\n",
    "2. **Compute Residuals**: The residuals (differences) between the predicted and actual values are computed.\n",
    "3. **Train a Weak Learner**: A weak learner (typically a decision tree) is trained to predict the residuals instead of the actual values.\n",
    "4. **Update Predictions**: The predictions of the model are updated using the predictions of the weak learner.\n",
    "5. **Repeat**: Steps 2-4 are repeated for a specified number of iterations.\n",
    "\n",
    "The final model is a weighted sum of the weak learners. Each weak learner in the ensemble focuses on the errors of the previous one, thereby improving the predictions of the model with each iteration.\n",
    "\n",
    "In contrast to AdaBoost, the weights of the training instances are not tweaked, instead, each predictor is trained using the residual errors of the predecessor as labels. There is a technique called the Gradient Boosted Trees whose base learner is CART (Classification and Regression Trees).\n",
    "\n",
    "There is an important parameter used in this technique known as Shrinkage. Shrinkage refers to the fact that the prediction of each tree in the ensemble is shrunk after it is multiplied by the learning rate (eta) which ranges between 0 to 1. There is a trade-off between eta and the number of estimators, decreasing learning rate needs to be compensated with increasing estimators in order to reach certain model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4082937-b177-4d49-b495-a8c4ad6b5fd4",
   "metadata": {},
   "source": [
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "--\n",
    "----\n",
    "\n",
    "The mathematical intuition behind the Gradient Boosting algorithm can be understood through the following steps:\n",
    "\n",
    "1. **Initialize the Model**: The algorithm starts by predicting the average of the target variable, which can be represented mathematically as $$F_0(x) = \\frac{1}{N}\\sum_{i=1}^{N}y_i$$ where $$F_0(x)$$ is the initial prediction, $$N$$ is the number of instances, and $$y_i$$ is the target value for the $$i^{th}$$ instance.\n",
    "\n",
    "2. **Compute Residuals**: The residuals (differences) between the predicted and actual values are computed. The residual for the $$i^{th}$$ instance can be calculated as $$r_{i1} = y_i - F_0(x_i)$$ where $$r_{i1}$$ is the residual, $$y_i$$ is the actual value, and $$F_0(x_i)$$ is the predicted value.\n",
    "\n",
    "3. **Train a Weak Learner**: A weak learner (typically a decision tree) is trained to predict these residuals instead of the actual values. This can be represented as $$h_1(x_i) = r_{i1}$$ where $$h_1(x_i)$$ is the prediction of the first weak learner.\n",
    "\n",
    "4. **Update Predictions**: The predictions of the model are updated using the predictions of the weak learner. The updated prediction can be calculated as $$F_1(x_i) = F_0(x_i) + \\nu h_1(x_i)$$ where $$F_1(x_i)$$ is the updated prediction, $$F_0(x_i)$$ is the old prediction, $$\\nu$$ is the learning rate, and $$h_1(x_i)$$ is the prediction of the weak learner.\n",
    "\n",
    "5. **Repeat**: Steps 2-4 are repeated for a specified number of iterations. With each iteration, a new weak learner is trained to correct the residuals of the previous weak learner, and the predictions are updated accordingly.\n",
    "\n",
    "6. **Final Model**: The final model is a weighted sum of all the weak learners. It can be represented as $$F_M(x) = F_0(x) + \\sum_{m=1}^{M}\\nu h_m(x)$$ where $$F_M(x)$$ is the final prediction, $$F_0(x)$$ is the initial prediction, $$\\nu$$ is the learning rate, $$h_m(x)$$ is the prediction of the $$m^{th}$$ weak learner, and $$M$$ is the total number of weak learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9512be-8420-4730-959c-e3a27c98ae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
